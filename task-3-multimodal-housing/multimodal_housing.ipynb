{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194c7792",
   "metadata": {},
   "source": [
    "# Task 3: Multimodal Housing Price Prediction (Images + Tabular Data)\n",
    "\n",
    "## Objective\n",
    "The objective of this task is to predict housing prices by combining visual information from house images with structured tabular features such as square footage, number of bedrooms, bathrooms, age of the house, and a location score.\n",
    "\n",
    "This task demonstrates a **multimodal machine learning approach**, where features extracted from images using a Convolutional Neural Network (CNN) are fused with traditional tabular data to improve regression performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Description\n",
    "The dataset consists of:\n",
    "- **Tabular features**:\n",
    "  - `sqft`: Total area of the house in square feet\n",
    "  - `bedrooms`: Number of bedrooms\n",
    "  - `bathrooms`: Number of bathrooms\n",
    "  - `age`: Age of the house in years\n",
    "  - `location_score`: A proxy score representing neighborhood quality\n",
    "- **Image data**:\n",
    "  - Synthetic house images generated programmatically, each corresponding to one data row\n",
    "- **Target variable**:\n",
    "  - `price`: House price (regression target)\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "1. A **pretrained MobileNetV2 CNN** is used as a fixed feature extractor to obtain visual embeddings from house images.\n",
    "2. Tabular features are standardized using `StandardScaler`.\n",
    "3. Image embeddings and tabular features are **concatenated** to form a unified feature vector.\n",
    "4. A fully connected neural network is trained on the combined features to predict house prices.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics\n",
    "Model performance is evaluated using:\n",
    "- **Mean Absolute Error (MAE)**\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "These metrics are commonly used for regression problems and provide insight into prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9f15f",
   "metadata": {},
   "source": [
    "Cell 1:- Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d450af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad6005",
   "metadata": {},
   "source": [
    "Cell 2:- Load Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60aaf4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>age</th>\n",
       "      <th>location_score</th>\n",
       "      <th>image</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>house_1.png</td>\n",
       "      <td>671570.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1603</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>house_2.png</td>\n",
       "      <td>338166.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3371</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>house_3.png</td>\n",
       "      <td>810195.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>730</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>house_4.png</td>\n",
       "      <td>249545.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2669</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>house_5.png</td>\n",
       "      <td>600407.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft  bedrooms  bathrooms  age  location_score        image      price\n",
       "0  3219         1          1   47               5  house_1.png  671570.71\n",
       "1  1603         2          2   47               2  house_2.png  338166.04\n",
       "2  3371         6          1   37               7  house_3.png  810195.33\n",
       "3   730         1          1   13               4  house_4.png  249545.45\n",
       "4  2669         5          1   35               4  house_5.png  600407.70"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/housing.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcf395",
   "metadata": {},
   "source": [
    "Cell 3:- CNN Feature Extractor (Pretrained MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285e3f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    pooling=\"avg\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return base_model.predict(img, verbose=0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd632f",
   "metadata": {},
   "source": [
    "Cell 4:- Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd25103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1280)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features = []\n",
    "\n",
    "for img_name in df[\"image\"]:\n",
    "    path = os.path.join(\"data/images\", img_name)\n",
    "    image_features.append(extract_image_features(path))\n",
    "\n",
    "image_features = np.array(image_features)\n",
    "image_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc36027",
   "metadata": {},
   "source": [
    "Cell 5:- Tabular Preprocessing + Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711c29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tabular = df[[\"sqft\", \"bedrooms\", \"bathrooms\", \"age\", \"location_score\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tabular = scaler.fit_transform(X_tabular)\n",
    "\n",
    "X = np.concatenate([X_tabular, image_features], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355e36c",
   "metadata": {},
   "source": [
    "Cell 6:- Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a80d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAROON UR RASHEED\\.conda\\envs\\tf\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - loss: 301876740096.0000 - val_loss: 251194769408.0000\n",
      "Epoch 2/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 301865828352.0000 - val_loss: 251181760512.0000\n",
      "Epoch 3/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 301850230784.0000 - val_loss: 251163213824.0000\n",
      "Epoch 4/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 301828898816.0000 - val_loss: 251138146304.0000\n",
      "Epoch 5/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 301801046016.0000 - val_loss: 251105542144.0000\n",
      "Epoch 6/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 301763493888.0000 - val_loss: 251063861248.0000\n",
      "Epoch 7/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 301715947520.0000 - val_loss: 251011694592.0000\n",
      "Epoch 8/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 301657849856.0000 - val_loss: 250947305472.0000\n",
      "Epoch 9/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 301584056320.0000 - val_loss: 250869219328.0000\n",
      "Epoch 10/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 301499809792.0000 - val_loss: 250775896064.0000\n",
      "Epoch 11/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 301393084416.0000 - val_loss: 250666057728.0000\n",
      "Epoch 12/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 301270630400.0000 - val_loss: 250536034304.0000\n",
      "Epoch 13/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 301126156288.0000 - val_loss: 250384269312.0000\n",
      "Epoch 14/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 300959760384.0000 - val_loss: 250208845824.0000\n",
      "Epoch 15/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 300765970432.0000 - val_loss: 250006732800.0000\n",
      "Epoch 16/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 300544131072.0000 - val_loss: 249774784512.0000\n",
      "Epoch 17/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 300296536064.0000 - val_loss: 249511641088.0000\n",
      "Epoch 18/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 300000051200.0000 - val_loss: 249217155072.0000\n",
      "Epoch 19/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 299677351936.0000 - val_loss: 248884133888.0000\n",
      "Epoch 20/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 299301699584.0000 - val_loss: 248511037440.0000\n",
      "Epoch 21/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 298900389888.0000 - val_loss: 248092819456.0000\n",
      "Epoch 22/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 298450550784.0000 - val_loss: 247628595200.0000\n",
      "Epoch 23/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 297934848000.0000 - val_loss: 247119003648.0000\n",
      "Epoch 24/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 297380184064.0000 - val_loss: 246558195712.0000\n",
      "Epoch 25/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 296750546944.0000 - val_loss: 245945778176.0000\n",
      "Epoch 26/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 296092532736.0000 - val_loss: 245273378816.0000\n",
      "Epoch 27/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 295372554240.0000 - val_loss: 244542914560.0000\n",
      "Epoch 28/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 294566526976.0000 - val_loss: 243751813120.0000\n",
      "Epoch 29/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 293693423616.0000 - val_loss: 242893586432.0000\n",
      "Epoch 30/30\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 292765302784.0000 - val_loss: 241961222144.0000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=8,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad232f",
   "metadata": {},
   "source": [
    "Cell 7:- Model Evaluation (MAE & RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb69127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "MAE: 552301.7100130208\n",
      "RMSE: 578765.7531320207\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
